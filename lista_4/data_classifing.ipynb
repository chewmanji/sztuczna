{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09e6d1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4789ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a204aa",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9315da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./raw_filled_data.csv\")\n",
    "\n",
    "std_data = pd.read_csv(\"./standardized_data.csv\")\n",
    "\n",
    "norm_data = pd.read_csv(\"./normalized_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bee759",
   "metadata": {},
   "source": [
    "# Test classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d393560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for Raw Data ===\n",
      "Naive Bayes Accuracy: 0.6268\n",
      "Decision Tree Accuracy: 0.7840\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.49      0.59        73\n",
      "           2       0.81      0.64      0.72       114\n",
      "           3       0.30      0.54      0.39        13\n",
      "           4       0.62      0.84      0.71        19\n",
      "           5       0.30      0.82      0.44        17\n",
      "           6       0.86      0.70      0.77        63\n",
      "           7       0.79      0.63      0.70        54\n",
      "           8       0.56      0.94      0.70        16\n",
      "           9       0.31      0.83      0.45        12\n",
      "          10       0.47      0.40      0.43        45\n",
      "\n",
      "    accuracy                           0.63       426\n",
      "   macro avg       0.58      0.68      0.59       426\n",
      "weighted avg       0.70      0.63      0.64       426\n",
      "\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.75      0.75        73\n",
      "           2       0.85      0.82      0.83       114\n",
      "           3       0.57      0.62      0.59        13\n",
      "           4       0.80      0.63      0.71        19\n",
      "           5       0.59      0.59      0.59        17\n",
      "           6       0.77      0.89      0.82        63\n",
      "           7       0.76      0.72      0.74        54\n",
      "           8       0.82      0.88      0.85        16\n",
      "           9       0.83      0.83      0.83        12\n",
      "          10       0.84      0.82      0.83        45\n",
      "\n",
      "    accuracy                           0.78       426\n",
      "   macro avg       0.76      0.75      0.75       426\n",
      "weighted avg       0.79      0.78      0.78       426\n",
      "\n",
      "\n",
      "=== Results for Standardized Data ===\n",
      "Naive Bayes Accuracy: 0.6244\n",
      "Decision Tree Accuracy: 0.7840\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.45      0.55        73\n",
      "           2       0.75      0.82      0.78       114\n",
      "           3       0.44      0.62      0.52        13\n",
      "           4       0.75      0.63      0.69        19\n",
      "           5       0.36      0.88      0.51        17\n",
      "           6       0.96      0.37      0.53        63\n",
      "           7       0.66      0.83      0.74        54\n",
      "           8       0.81      0.81      0.81        16\n",
      "           9       0.24      0.75      0.36        12\n",
      "          10       0.44      0.31      0.36        45\n",
      "\n",
      "    accuracy                           0.62       426\n",
      "   macro avg       0.61      0.65      0.59       426\n",
      "weighted avg       0.69      0.62      0.62       426\n",
      "\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.75      0.75        73\n",
      "           2       0.85      0.82      0.83       114\n",
      "           3       0.57      0.62      0.59        13\n",
      "           4       0.75      0.63      0.69        19\n",
      "           5       0.59      0.59      0.59        17\n",
      "           6       0.77      0.89      0.82        63\n",
      "           7       0.76      0.72      0.74        54\n",
      "           8       0.82      0.88      0.85        16\n",
      "           9       0.83      0.83      0.83        12\n",
      "          10       0.86      0.82      0.84        45\n",
      "\n",
      "    accuracy                           0.78       426\n",
      "   macro avg       0.76      0.75      0.75       426\n",
      "weighted avg       0.79      0.78      0.78       426\n",
      "\n",
      "\n",
      "=== Results for Normalized Data ===\n",
      "Naive Bayes Accuracy: 0.6338\n",
      "Decision Tree Accuracy: 0.7864\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.45      0.55        73\n",
      "           2       0.75      0.82      0.79       114\n",
      "           3       0.44      0.62      0.52        13\n",
      "           4       0.75      0.63      0.69        19\n",
      "           5       0.36      0.88      0.51        17\n",
      "           6       0.96      0.41      0.58        63\n",
      "           7       0.69      0.83      0.76        54\n",
      "           8       0.82      0.88      0.85        16\n",
      "           9       0.24      0.75      0.36        12\n",
      "          10       0.44      0.31      0.36        45\n",
      "\n",
      "    accuracy                           0.63       426\n",
      "   macro avg       0.62      0.66      0.60       426\n",
      "weighted avg       0.70      0.63      0.63       426\n",
      "\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.75      0.75        73\n",
      "           2       0.85      0.82      0.83       114\n",
      "           3       0.57      0.62      0.59        13\n",
      "           4       0.75      0.63      0.69        19\n",
      "           5       0.59      0.59      0.59        17\n",
      "           6       0.77      0.90      0.83        63\n",
      "           7       0.78      0.72      0.75        54\n",
      "           8       0.82      0.88      0.85        16\n",
      "           9       0.83      0.83      0.83        12\n",
      "          10       0.84      0.82      0.83        45\n",
      "\n",
      "    accuracy                           0.79       426\n",
      "   macro avg       0.76      0.76      0.75       426\n",
      "weighted avg       0.79      0.79      0.79       426\n",
      "\n",
      "\n",
      "=== SUMMARY COMPARISON ===\n",
      "Raw Data:\n",
      "  Naive Bayes: 0.6268\n",
      "  Decision Tree: 0.7840\n",
      "Standardized Data:\n",
      "  Naive Bayes: 0.6244\n",
      "  Decision Tree: 0.7840\n",
      "Normalized Data:\n",
      "  Naive Bayes: 0.6338\n",
      "  Decision Tree: 0.7864\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(data):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, data_type):\n",
    "    print(f\"\\n=== Results for {data_type} ===\")\n",
    "    \n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "    \n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "    \n",
    "    print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "    print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nNaive Bayes Classification Report:\")\n",
    "    print(classification_report(y_test, nb_pred))\n",
    "    \n",
    "    print(\"\\nDecision Tree Classification Report:\")\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    \n",
    "    return nb, dt, nb_accuracy, dt_accuracy\n",
    "\n",
    "datasets = {\n",
    "    'Raw Data': raw_data,\n",
    "    'Standardized Data': std_data,\n",
    "    'Normalized Data': norm_data\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    X_train, X_test, y_train, y_test = prepare_data(data)\n",
    "    nb_model, dt_model, nb_acc, dt_acc = train_and_evaluate(X_train, X_test, y_train, y_test, name)\n",
    "    results[name] = {\n",
    "        'nb_model': nb_model,\n",
    "        'dt_model': dt_model,\n",
    "        'nb_accuracy': nb_acc,\n",
    "        'dt_accuracy': dt_acc\n",
    "    }\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n=== SUMMARY COMPARISON ===\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Naive Bayes: {result['nb_accuracy']:.4f}\")\n",
    "    print(f\"  Decision Tree: {result['dt_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f1d76",
   "metadata": {},
   "source": [
    "We can see that Decision Tree Classifier (DT) is doing better the Naive Bayes Classifier (NB) - one of the reason might be that in dataset there are some features which are not normally distributed.<br>\n",
    "\n",
    "The class imbalance also has a impact on the ability to predict for NB - with many more healthy cases, the algorithm develops a strong bias toward predicting the majority class. The statistics shows it - dangerous cases (CLASS value <8-10>) are fairly worse predicted than the more common cases. DTs can be more robust here because they focus on local patterns within subsets of data. However, there's a risk that the DT will start overfitting.<br>\n",
    "\n",
    "NB often shows higher recall than precision, especially for minority classes. This suggests it has a lower decision threshold - it's more willing to predict a class, leading to more false positives.<br>\n",
    "DT generally maintains better precision-recall balance, indicating more conservative and accurate predictions.\n",
    "\n",
    "\n",
    "Another reason why NB is doing worse is feature interactions in CTG dataset. NB assumes feature independence, so it can't capture these interactions directly. It treats each feature separately when calculating probabilities.\n",
    "\n",
    "There are no big differences between the datasets since methods used to preprocess data (standardization, normalization) didn't change the distribution of features. Probably using PCA or selecting features would improve results for NB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a060351",
   "metadata": {},
   "source": [
    "# Hyperparameters finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1caf8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Raw Data ====================\n",
      "\n",
      "--- Gaussian Naive Bayes on Raw Data ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'var_smoothing': np.float64(1e-09)}\n",
      "Best CV score: 0.5947\n",
      "Test accuracy: 0.6268\n",
      "\n",
      "--- Decision Tree on Raw Data ---\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best CV score: 0.8041\n",
      "Test accuracy: 0.7911\n",
      "\n",
      "==================== Standardized Data ====================\n",
      "\n",
      "--- Gaussian Naive Bayes on Standardized Data ---\n",
      "Best parameters: {'var_smoothing': np.float64(0.004328761281083057)}\n",
      "Best CV score: 0.6994\n",
      "Test accuracy: 0.7136\n",
      "\n",
      "--- Decision Tree on Standardized Data ---\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best CV score: 0.8059\n",
      "Test accuracy: 0.7911\n",
      "\n",
      "==================== Normalized Data ====================\n",
      "\n",
      "--- Gaussian Naive Bayes on Normalized Data ---\n",
      "Best parameters: {'var_smoothing': np.float64(0.004328761281083057)}\n",
      "Best CV score: 0.7088\n",
      "Test accuracy: 0.7183\n",
      "\n",
      "--- Decision Tree on Normalized Data ---\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best CV score: 0.8041\n",
      "Test accuracy: 0.7911\n",
      "\n",
      "============================================================\n",
      "SUMMARY COMPARISON\n",
      "============================================================\n",
      "Dataset              Algorithm       CV Score   Test Score\n",
      "------------------------------------------------------------\n",
      "Raw Data             Naive Bayes     0.5947     0.6268    \n",
      "Raw Data             Decision Tree   0.8041     0.7911    \n",
      "Standardized Data    Naive Bayes     0.6994     0.7136    \n",
      "Standardized Data    Decision Tree   0.8059     0.7911    \n",
      "Normalized Data      Naive Bayes     0.7088     0.7183    \n",
      "Normalized Data      Decision Tree   0.8041     0.7911    \n",
      "\n",
      "Best combination: Decision Tree on Raw Data\n",
      "Test accuracy: 0.7911\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "nb_param_grid = {\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "dt_param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    print(f\"\\n{'='*20} {dataset_name} {'='*20}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = prepare_data(data)\n",
    "    \n",
    "    results[dataset_name] = {}\n",
    "    \n",
    "    print(f\"\\n--- Gaussian Naive Bayes on {dataset_name} ---\")\n",
    "    \n",
    "    nb_grid_search = GridSearchCV(\n",
    "        estimator=GaussianNB(),\n",
    "        param_grid=nb_param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    nb_grid_search.fit(X_train, y_train)\n",
    "    nb_best_model = nb_grid_search.best_estimator_\n",
    "    nb_test_score = nb_best_model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Best parameters: {nb_grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {nb_grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test accuracy: {nb_test_score:.4f}\")\n",
    "    \n",
    "    results[dataset_name]['NB'] = {\n",
    "        'best_params': nb_grid_search.best_params_,\n",
    "        'cv_score': nb_grid_search.best_score_,\n",
    "        'test_score': nb_test_score,\n",
    "        'model': nb_best_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Decision Tree on {dataset_name} ---\")\n",
    "    \n",
    "    dt_grid_search = GridSearchCV(\n",
    "        estimator=DecisionTreeClassifier(random_state=42),\n",
    "        param_grid=dt_param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    dt_grid_search.fit(X_train, y_train)\n",
    "    dt_best_model = dt_grid_search.best_estimator_\n",
    "    dt_test_score = dt_best_model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Best parameters: {dt_grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {dt_grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test accuracy: {dt_test_score:.4f}\")\n",
    "    \n",
    "    results[dataset_name]['DT'] = {\n",
    "        'best_params': dt_grid_search.best_params_,\n",
    "        'cv_score': dt_grid_search.best_score_,\n",
    "        'test_score': dt_test_score,\n",
    "        'model': dt_best_model\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"{'Dataset':<20} {'Algorithm':<15} {'CV Score':<10} {'Test Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    for algo in ['NB', 'DT']:\n",
    "        cv_score = results[dataset_name][algo]['cv_score']\n",
    "        test_score = results[dataset_name][algo]['test_score']\n",
    "        algo_name = 'Naive Bayes' if algo == 'NB' else 'Decision Tree'\n",
    "        print(f\"{dataset_name:<20} {algo_name:<15} {cv_score:<10.4f} {test_score:<10.4f}\")\n",
    "\n",
    "best_combo = None\n",
    "best_score = 0\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    for algo in ['NB', 'DT']:\n",
    "        test_score = results[dataset_name][algo]['test_score']\n",
    "        if test_score > best_score:\n",
    "            best_score = test_score\n",
    "            best_combo = (dataset_name, algo)\n",
    "\n",
    "if best_combo:\n",
    "    dataset_name, algo = best_combo\n",
    "    algo_name = 'Naive Bayes' if algo == 'NB' else 'Decision Tree'\n",
    "    print(f\"\\nBest combination: {algo_name} on {dataset_name}\")\n",
    "    print(f\"Test accuracy: {best_score:.4f}\")\n",
    "    print(f\"Best parameters: {results[dataset_name][algo]['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb957ea0",
   "metadata": {},
   "source": [
    "Finetuning the smoothing hyperparameter increased the NB accuracy by few %.<br>\n",
    "Finetuning parameters in DT didn't improve the accuracy significantly.<br>\n",
    "For each dataset the parameters stay the same for DT - that's because of DT is scale-invariant.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
